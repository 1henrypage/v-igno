{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian IGNO: HMC Posterior Sampling Experiment\n",
    "\n",
    "This notebook implements Hamiltonian Monte Carlo (via Pyro's NUTS) for uncertainty quantification in IGNO inverse problems.\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "Instead of finding a point estimate $\\beta^* = \\arg\\min F(\\beta)$, we **sample** from the posterior:\n",
    "\n",
    "$$p(\\beta | u_{obs}) \\propto \\exp(-U(\\beta))$$\n",
    "\n",
    "where the potential energy is:\n",
    "\n",
    "$$U(\\beta) = \\underbrace{\\rho_{data} \\cdot L_{data}(\\beta)}_{\\text{likelihood}} + \\underbrace{\\rho_{pde} \\cdot L_{pde}(\\beta)}_{\\text{physics prior}} + \\underbrace{(-\\log p_{NF}(\\beta))}_{\\text{flow prior}}$$\n",
    "\n",
    "The flow prior term $-\\log p_{NF}(\\beta) = \\frac{1}{2}\\|z\\|^2 - \\log|\\det J|$ where $z = NF(\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (adjust as needed)\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# Pyro for NUTS\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "# Our modules\n",
    "from src.solver.config import TrainingConfig\n",
    "from src.problems import create_problem\n",
    "\n",
    "# Plotting setup\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Pyro: {pyro.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHECKPOINT_PATH = PROJECT_ROOT / \"runs\" / \"18_dims\" / \"weights\" / \"best.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "# You'll need a config file or create one programmatically\n",
    "# For now, let's create a minimal config\n",
    "config_dict = {\n",
    "    'problem': {\n",
    "        'type': 'darcy_continuous',\n",
    "        'train_data': None,  # Not needed for evaluation\n",
    "        'test_data': str(PROJECT_ROOT / 'data' / 'darcy' / 'test'),  # Adjust path\n",
    "    },\n",
    "    'device': DEVICE,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create problem and load checkpoint\n",
    "# Note: You may need to adjust this based on your actual config setup\n",
    "\n",
    "config = TrainingConfig.from_dict(config_dict)\n",
    "problem = create_problem(config, load_train_data=False)\n",
    "\n",
    "# Load pretrained weights\n",
    "problem.load_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "# Freeze all models\n",
    "problem.eval_mode()\n",
    "for name, model in problem.model_dict.items():\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"{name}: {sum(p.numel() for p in model.parameters()):,} params (frozen)\")\n",
    "\n",
    "print(f\"\\nLatent dimension: {problem.BETA_SIZE}\")\n",
    "print(f\"Test samples available: {problem.get_n_test_samples()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Test Sample and Prepare Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a single test sample to work with\n",
    "SAMPLE_IDX = 0  # Change this to experiment with different samples\n",
    "\n",
    "# Observation setup (matching your evaluation config)\n",
    "N_OBS = 100  # Number of observation points\n",
    "SNR_DB = 30  # Signal-to-noise ratio (None for clean observations)\n",
    "\n",
    "# Sample observation indices\n",
    "n_points = problem.get_n_points()\n",
    "obs_indices = problem.sample_observation_indices(\n",
    "    n_total=n_points,\n",
    "    n_obs=N_OBS,\n",
    "    method='random'\n",
    ")\n",
    "\n",
    "# Prepare observations for this sample\n",
    "obs_data = problem.prepare_observations(\n",
    "    sample_indices=[SAMPLE_IDX],\n",
    "    obs_indices=obs_indices,\n",
    "    snr_db=SNR_DB\n",
    ")\n",
    "\n",
    "# Extract tensors (squeeze batch dimension since we're doing single sample)\n",
    "x_obs = obs_data['x_obs']      # (1, n_obs, 2)\n",
    "u_obs = obs_data['u_obs']      # (1, n_obs, 1)\n",
    "x_full = obs_data['x_full']    # (1, n_points, 2)\n",
    "u_true = obs_data['u_true']    # (1, n_points, 1)\n",
    "a_true = obs_data['a_true']    # (1, n_points, 1)\n",
    "\n",
    "print(f\"Sample index: {SAMPLE_IDX}\")\n",
    "print(f\"Observation points: {N_OBS} / {n_points}\")\n",
    "print(f\"SNR: {SNR_DB} dB\" if SNR_DB else \"Clean observations\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  x_obs: {x_obs.shape}\")\n",
    "print(f\"  u_obs: {u_obs.shape}\")\n",
    "print(f\"  x_full: {x_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the true coefficient field and observation locations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Reshape for plotting (assuming 29x29 grid)\n",
    "grid_size = int(np.sqrt(n_points))\n",
    "a_true_2d = a_true[0].cpu().reshape(grid_size, grid_size)\n",
    "u_true_2d = u_true[0].cpu().reshape(grid_size, grid_size)\n",
    "\n",
    "# True coefficient\n",
    "im0 = axes[0].imshow(a_true_2d, origin='lower', extent=[0, 1, 0, 1], cmap='viridis')\n",
    "axes[0].set_title('True Coefficient $a(x)$')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# True solution\n",
    "im1 = axes[1].imshow(u_true_2d, origin='lower', extent=[0, 1, 0, 1], cmap='coolwarm')\n",
    "axes[1].set_title('True Solution $u(x)$')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# Observation locations\n",
    "x_obs_np = x_obs[0].cpu().numpy()\n",
    "axes[2].scatter(x_obs_np[:, 0], x_obs_np[:, 1], c='red', s=10, alpha=0.7)\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].set_title(f'Observation Locations (n={N_OBS})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Potential Energy Function\n",
    "\n",
    "The potential energy (negative log-posterior) is:\n",
    "\n",
    "$$U(\\beta) = w_{data} \\cdot L_{data}(\\beta) + w_{pde} \\cdot L_{pde}(\\beta) - \\log p_{NF}(\\beta)$$\n",
    "\n",
    "Where:\n",
    "- $L_{data}$ = data mismatch loss (relative L2)\n",
    "- $L_{pde}$ = PDE residual loss  \n",
    "- $\\log p_{NF}(\\beta)$ = log probability from normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights (same as your evaluation config)\n",
    "W_DATA = 1.0\n",
    "W_PDE = 1.0\n",
    "\n",
    "def compute_potential_energy(beta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute U(β) = -log p(β | u_obs) + const\n",
    "    \n",
    "    Args:\n",
    "        beta: Latent representation (1, latent_dim) or (latent_dim,)\n",
    "        \n",
    "    Returns:\n",
    "        Scalar potential energy\n",
    "    \"\"\"\n",
    "    # Ensure batch dimension\n",
    "    if beta.dim() == 1:\n",
    "        beta = beta.unsqueeze(0)\n",
    "    \n",
    "    # Data loss: ||u_pred - u_obs||² / ||u_obs||² (relative L2)\n",
    "    loss_data = problem.loss_data_from_beta(beta, x_obs, u_obs, target_type='u')\n",
    "    \n",
    "    # PDE loss: weak form residual\n",
    "    loss_pde = problem.loss_pde_from_beta(beta)\n",
    "    \n",
    "    # Flow prior: -log p_NF(β)\n",
    "    # log_prob_latent returns log p(β), so we negate it\n",
    "    log_prior = problem.log_prob_latent(beta)  # (batch,)\n",
    "    neg_log_prior = -log_prior.mean()  # Scalar\n",
    "    \n",
    "    # Total potential energy\n",
    "    U = W_DATA * loss_data + W_PDE * loss_pde + neg_log_prior\n",
    "    \n",
    "    return U\n",
    "\n",
    "\n",
    "def compute_log_prob(beta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log p(β | u_obs) = -U(β) for Pyro.\n",
    "    \n",
    "    Pyro's NUTS expects log probability, not potential energy.\n",
    "    \"\"\"\n",
    "    return -compute_potential_energy(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the potential energy function\n",
    "print(\"Testing potential energy computation...\")\n",
    "\n",
    "# Sample a random beta from the NF prior\n",
    "beta_test = problem.sample_latent_from_nf(num_samples=1)\n",
    "print(f\"β shape: {beta_test.shape}\")\n",
    "print(f\"β range: [{beta_test.min():.3f}, {beta_test.max():.3f}]\")\n",
    "\n",
    "# Compute potential energy\n",
    "beta_test.requires_grad_(True)\n",
    "U = compute_potential_energy(beta_test)\n",
    "print(f\"\\nU(β) = {U.item():.4f}\")\n",
    "\n",
    "# Verify gradient exists\n",
    "U.backward()\n",
    "print(f\"∇U(β) norm: {beta_test.grad.norm().item():.4f}\")\n",
    "print(f\"∇U(β) range: [{beta_test.grad.min():.4f}, {beta_test.grad.max():.4f}]\")\n",
    "\n",
    "# Check individual loss components\n",
    "beta_test2 = problem.sample_latent_from_nf(num_samples=1)\n",
    "with torch.no_grad():\n",
    "    l_data = problem.loss_data_from_beta(beta_test2, x_obs, u_obs, 'u')\n",
    "    l_pde = problem.loss_pde_from_beta(beta_test2)\n",
    "    l_prior = -problem.log_prob_latent(beta_test2).mean()\n",
    "    \n",
    "print(f\"\\nLoss components:\")\n",
    "print(f\"  L_data: {l_data.item():.4f}\")\n",
    "print(f\"  L_pde:  {l_pde.item():.4f}\")\n",
    "print(f\"  -log p(β): {l_prior.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Point Estimation (Baseline)\n",
    "\n",
    "First, let's run the standard IGNO inversion to get a baseline point estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import IGNOInverter\n",
    "from src.solver.config import InversionConfig, LossWeights, OptimizerConfig\n",
    "\n",
    "# Create inversion config\n",
    "inv_config = InversionConfig(\n",
    "    epochs=500,\n",
    "    loss_weights=LossWeights(pde=W_PDE, data=W_DATA),\n",
    "    optimizer=OptimizerConfig(type='Adam', lr=0.01),\n",
    ")\n",
    "\n",
    "# Run IGNO inversion\n",
    "nf = problem.model_dict['nf']\n",
    "inverter = IGNOInverter(problem, nf)\n",
    "\n",
    "print(\"Running IGNO point estimation...\")\n",
    "beta_point = inverter.invert(\n",
    "    x_obs=x_obs,\n",
    "    u_obs=u_obs,\n",
    "    x_full=x_full,\n",
    "    config=inv_config,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nPoint estimate β shape: {beta_point.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from point estimate\n",
    "preds_point = problem.predict_from_beta(beta_point, x_full)\n",
    "a_point = preds_point['a_pred']  # (1, n_points, 1)\n",
    "u_point = preds_point['u_pred']  # (1, n_points, 1)\n",
    "\n",
    "# Compute metrics\n",
    "from src.evaluation import compute_all_metrics\n",
    "\n",
    "metrics_a_point = compute_all_metrics(a_point[0], a_true[0])\n",
    "metrics_u_point = compute_all_metrics(u_point[0], u_true[0])\n",
    "\n",
    "print(\"Point Estimate Metrics:\")\n",
    "print(f\"  Coefficient a: RMSE={metrics_a_point['rmse']:.6f}, RelL2={metrics_a_point['rel_l2']:.6f}\")\n",
    "print(f\"  Solution u:    RMSE={metrics_u_point['rmse']:.6f}, RelL2={metrics_u_point['rel_l2']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Pyro Model for NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = problem.BETA_SIZE\n",
    "\n",
    "def pyro_potential_fn(params):\n",
    "    \"\"\"\n",
    "    Potential function for Pyro's NUTS.\n",
    "    \n",
    "    Pyro NUTS with `potential_fn` expects a function that takes a dict\n",
    "    of parameters and returns the potential energy (negative log prob).\n",
    "    \"\"\"\n",
    "    beta = params['beta']\n",
    "    return compute_potential_energy(beta)\n",
    "\n",
    "\n",
    "def run_hmc_sampling(\n",
    "    num_samples: int = 500,\n",
    "    warmup_steps: int = 200,\n",
    "    num_chains: int = 1,\n",
    "    init_beta: torch.Tensor = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run NUTS sampling using Pyro.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of posterior samples to collect\n",
    "        warmup_steps: Number of warmup/adaptation steps\n",
    "        num_chains: Number of parallel chains\n",
    "        init_beta: Initial beta value (if None, sample from NF prior)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with samples and diagnostics\n",
    "    \"\"\"\n",
    "    # Initialize from NF prior if not provided\n",
    "    if init_beta is None:\n",
    "        init_beta = problem.sample_latent_from_nf(num_samples=1).squeeze(0)\n",
    "    \n",
    "    # Initial params dict\n",
    "    init_params = {'beta': init_beta.clone()}\n",
    "    \n",
    "    # Create NUTS kernel with potential function\n",
    "    nuts_kernel = NUTS(\n",
    "        potential_fn=pyro_potential_fn,\n",
    "        adapt_step_size=True,\n",
    "        adapt_mass_matrix=True,\n",
    "        full_mass=False,  # Diagonal mass matrix (faster)\n",
    "        max_tree_depth=10,\n",
    "        target_accept_prob=0.8,\n",
    "    )\n",
    "    \n",
    "    # Run MCMC\n",
    "    mcmc = MCMC(\n",
    "        nuts_kernel,\n",
    "        num_samples=num_samples,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_chains=num_chains,\n",
    "        initial_params=init_params,\n",
    "        disable_progbar=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"Running NUTS: {warmup_steps} warmup + {num_samples} samples\")\n",
    "    mcmc.run()\n",
    "    \n",
    "    # Get samples\n",
    "    samples = mcmc.get_samples()\n",
    "    beta_samples = samples['beta']  # (num_samples, latent_dim)\n",
    "    \n",
    "    # Get diagnostics\n",
    "    diagnostics = {\n",
    "        'step_size': nuts_kernel.step_size,\n",
    "        'accept_prob': nuts_kernel.num_accepts / (num_samples + warmup_steps),\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'beta_samples': beta_samples,\n",
    "        'mcmc': mcmc,\n",
    "        'diagnostics': diagnostics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run NUTS Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any previous Pyro state\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Run NUTS\n",
    "# Start with fewer samples for testing, increase later\n",
    "results = run_hmc_sampling(\n",
    "    num_samples=500,    # Posterior samples to collect\n",
    "    warmup_steps=200,   # Adaptation phase\n",
    "    num_chains=1,\n",
    "    init_beta=beta_point.squeeze(0),  # Initialize at point estimate\n",
    ")\n",
    "\n",
    "beta_samples = results['beta_samples']\n",
    "print(f\"\\nCollected {beta_samples.shape[0]} samples\")\n",
    "print(f\"Sample shape: {beta_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MCMC diagnostics\n",
    "mcmc = results['mcmc']\n",
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Posterior Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots for first few dimensions\n",
    "n_dims_to_plot = min(6, LATENT_DIM)\n",
    "fig, axes = plt.subplots(2, n_dims_to_plot, figsize=(15, 6))\n",
    "\n",
    "for i in range(n_dims_to_plot):\n",
    "    # Trace plot\n",
    "    axes[0, i].plot(beta_samples[:, i].cpu().numpy(), alpha=0.7)\n",
    "    axes[0, i].axhline(beta_point[0, i].cpu().item(), color='r', linestyle='--', label='Point est.')\n",
    "    axes[0, i].set_title(f'β[{i}] trace')\n",
    "    axes[0, i].set_xlabel('Sample')\n",
    "    \n",
    "    # Histogram\n",
    "    axes[1, i].hist(beta_samples[:, i].cpu().numpy(), bins=30, density=True, alpha=0.7)\n",
    "    axes[1, i].axvline(beta_point[0, i].cpu().item(), color='r', linestyle='--', label='Point est.')\n",
    "    axes[1, i].set_title(f'β[{i}] posterior')\n",
    "    axes[1, i].set_xlabel('Value')\n",
    "\n",
    "axes[0, 0].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior statistics for β\n",
    "beta_mean = beta_samples.mean(dim=0)\n",
    "beta_std = beta_samples.std(dim=0)\n",
    "\n",
    "print(\"Posterior statistics for β:\")\n",
    "print(f\"  Mean: {beta_mean[:5].cpu().numpy()} ...\")\n",
    "print(f\"  Std:  {beta_std[:5].cpu().numpy()} ...\")\n",
    "print(f\"\\n  Point estimate: {beta_point[0, :5].cpu().numpy()} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transform to Coefficient Field Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode all β samples to coefficient fields\n",
    "# This might take a moment for many samples\n",
    "\n",
    "print(f\"Decoding {len(beta_samples)} β samples to coefficient fields...\")\n",
    "\n",
    "a_samples = []\n",
    "u_samples = []\n",
    "\n",
    "batch_size = 50  # Process in batches for efficiency\n",
    "with torch.no_grad():\n",
    "    for i in trange(0, len(beta_samples), batch_size, desc=\"Decoding\"):\n",
    "        batch_beta = beta_samples[i:i+batch_size]\n",
    "        # Expand x_full to match batch size\n",
    "        batch_x = x_full.expand(len(batch_beta), -1, -1)\n",
    "        \n",
    "        preds = problem.predict_from_beta(batch_beta, batch_x)\n",
    "        a_samples.append(preds['a_pred'])\n",
    "        u_samples.append(preds['u_pred'])\n",
    "\n",
    "a_samples = torch.cat(a_samples, dim=0)  # (n_samples, n_points, 1)\n",
    "u_samples = torch.cat(u_samples, dim=0)\n",
    "\n",
    "print(f\"\\na_samples shape: {a_samples.shape}\")\n",
    "print(f\"u_samples shape: {u_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior statistics for coefficient field\n",
    "a_posterior_mean = a_samples.mean(dim=0)  # (n_points, 1)\n",
    "a_posterior_std = a_samples.std(dim=0)\n",
    "\n",
    "u_posterior_mean = u_samples.mean(dim=0)\n",
    "u_posterior_std = u_samples.std(dim=0)\n",
    "\n",
    "# Also get quantiles for credible intervals\n",
    "a_lower = torch.quantile(a_samples, 0.025, dim=0)\n",
    "a_upper = torch.quantile(a_samples, 0.975, dim=0)\n",
    "\n",
    "print(\"Posterior statistics computed!\")\n",
    "print(f\"  a mean range: [{a_posterior_mean.min():.4f}, {a_posterior_mean.max():.4f}]\")\n",
    "print(f\"  a std range:  [{a_posterior_std.min():.4f}, {a_posterior_std.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for 2D plotting\n",
    "def to_2d(tensor, size=grid_size):\n",
    "    return tensor.cpu().squeeze().reshape(size, size)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Row 1: Coefficient field a\n",
    "vmin_a, vmax_a = a_true[0].min().item(), a_true[0].max().item()\n",
    "\n",
    "im00 = axes[0, 0].imshow(to_2d(a_true[0]), origin='lower', extent=[0,1,0,1], vmin=vmin_a, vmax=vmax_a, cmap='viridis')\n",
    "axes[0, 0].set_title('True $a(x)$')\n",
    "plt.colorbar(im00, ax=axes[0, 0])\n",
    "\n",
    "im01 = axes[0, 1].imshow(to_2d(a_point[0]), origin='lower', extent=[0,1,0,1], vmin=vmin_a, vmax=vmax_a, cmap='viridis')\n",
    "axes[0, 1].set_title('Point Estimate $a(x)$')\n",
    "plt.colorbar(im01, ax=axes[0, 1])\n",
    "\n",
    "im02 = axes[0, 2].imshow(to_2d(a_posterior_mean), origin='lower', extent=[0,1,0,1], vmin=vmin_a, vmax=vmax_a, cmap='viridis')\n",
    "axes[0, 2].set_title('Posterior Mean $a(x)$')\n",
    "plt.colorbar(im02, ax=axes[0, 2])\n",
    "\n",
    "im03 = axes[0, 3].imshow(to_2d(a_posterior_std), origin='lower', extent=[0,1,0,1], cmap='hot')\n",
    "axes[0, 3].set_title('Posterior Std $a(x)$')\n",
    "plt.colorbar(im03, ax=axes[0, 3])\n",
    "\n",
    "# Row 2: Solution field u\n",
    "vmin_u, vmax_u = u_true[0].min().item(), u_true[0].max().item()\n",
    "\n",
    "im10 = axes[1, 0].imshow(to_2d(u_true[0]), origin='lower', extent=[0,1,0,1], vmin=vmin_u, vmax=vmax_u, cmap='coolwarm')\n",
    "axes[1, 0].set_title('True $u(x)$')\n",
    "plt.colorbar(im10, ax=axes[1, 0])\n",
    "\n",
    "im11 = axes[1, 1].imshow(to_2d(u_point[0]), origin='lower', extent=[0,1,0,1], vmin=vmin_u, vmax=vmax_u, cmap='coolwarm')\n",
    "axes[1, 1].set_title('Point Estimate $u(x)$')\n",
    "plt.colorbar(im11, ax=axes[1, 1])\n",
    "\n",
    "im12 = axes[1, 2].imshow(to_2d(u_posterior_mean), origin='lower', extent=[0,1,0,1], vmin=vmin_u, vmax=vmax_u, cmap='coolwarm')\n",
    "axes[1, 2].set_title('Posterior Mean $u(x)$')\n",
    "plt.colorbar(im12, ax=axes[1, 2])\n",
    "\n",
    "im13 = axes[1, 3].imshow(to_2d(u_posterior_std), origin='lower', extent=[0,1,0,1], cmap='hot')\n",
    "axes[1, 3].set_title('Posterior Std $u(x)$')\n",
    "plt.colorbar(im13, ax=axes[1, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hmc_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some posterior samples (gallery view)\n",
    "n_gallery = 9\n",
    "sample_indices = np.linspace(0, len(a_samples)-1, n_gallery, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    im = axes[i].imshow(to_2d(a_samples[idx]), origin='lower', extent=[0,1,0,1], \n",
    "                        vmin=vmin_a, vmax=vmax_a, cmap='viridis')\n",
    "    axes[i].set_title(f'Sample {idx}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Posterior Samples of $a(x)$', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compute Bayesian Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare point estimate vs posterior mean\n",
    "metrics_a_posterior = compute_all_metrics(a_posterior_mean.unsqueeze(0), a_true[0])\n",
    "metrics_u_posterior = compute_all_metrics(u_posterior_mean.unsqueeze(0), u_true[0])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Point Estimate vs Bayesian Posterior Mean\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCoefficient a:\")\n",
    "print(f\"  Point estimate: RMSE={metrics_a_point['rmse']:.6f}, RelL2={metrics_a_point['rel_l2']:.6f}\")\n",
    "print(f\"  Posterior mean: RMSE={metrics_a_posterior['rmse']:.6f}, RelL2={metrics_a_posterior['rel_l2']:.6f}\")\n",
    "print(f\"\\nSolution u:\")\n",
    "print(f\"  Point estimate: RMSE={metrics_u_point['rmse']:.6f}, RelL2={metrics_u_point['rel_l2']:.6f}\")\n",
    "print(f\"  Posterior mean: RMSE={metrics_u_posterior['rmse']:.6f}, RelL2={metrics_u_posterior['rel_l2']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration check: Does the true value fall within credible intervals?\n",
    "# For well-calibrated UQ, ~95% of true values should be within 95% CI\n",
    "\n",
    "a_true_flat = a_true[0].squeeze().cpu()\n",
    "a_lower_flat = a_lower.squeeze().cpu()\n",
    "a_upper_flat = a_upper.squeeze().cpu()\n",
    "\n",
    "coverage = ((a_true_flat >= a_lower_flat) & (a_true_flat <= a_upper_flat)).float().mean()\n",
    "print(f\"\\n95% Credible Interval Coverage: {coverage.item()*100:.1f}%\")\n",
    "print(f\"  (Should be ~95% for well-calibrated uncertainty)\")\n",
    "\n",
    "# Average width of credible intervals\n",
    "ci_width = (a_upper_flat - a_lower_flat).mean()\n",
    "print(f\"\\nAverage 95% CI width: {ci_width.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps\n",
    "\n",
    "### What We Did\n",
    "1. Loaded a pretrained IGNO model (encoder, decoders, NF)\n",
    "2. Defined the potential energy $U(\\beta) = w_{data} L_{data} + w_{pde} L_{pde} - \\log p_{NF}(\\beta)$\n",
    "3. Ran NUTS sampling to get posterior samples $\\{\\beta^{(i)}\\}$\n",
    "4. Decoded to coefficient field samples $\\{a^{(i)}(x)\\}$\n",
    "5. Computed posterior statistics (mean, std, credible intervals)\n",
    "\n",
    "### Key Results\n",
    "- **Posterior mean** often similar to point estimate (sanity check ✓)\n",
    "- **Posterior std** gives spatial uncertainty map\n",
    "- **Coverage** tells us if uncertainty is calibrated\n",
    "\n",
    "### Next Steps\n",
    "1. Run on multiple test samples to get aggregate statistics\n",
    "2. Compare performance across noise levels (SNR)\n",
    "3. Tune MCMC parameters (more samples, longer warmup)\n",
    "4. Implement proper Bayesian metrics (CRPS, calibration curves)\n",
    "5. Integrate into main evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later analysis\n",
    "results_to_save = {\n",
    "    'sample_idx': SAMPLE_IDX,\n",
    "    'n_obs': N_OBS,\n",
    "    'snr_db': SNR_DB,\n",
    "    'beta_samples': beta_samples.cpu(),\n",
    "    'beta_point': beta_point.cpu(),\n",
    "    'a_posterior_mean': a_posterior_mean.cpu(),\n",
    "    'a_posterior_std': a_posterior_std.cpu(),\n",
    "    'a_point': a_point.cpu(),\n",
    "    'a_true': a_true.cpu(),\n",
    "    'metrics_point': metrics_a_point,\n",
    "    'metrics_posterior': metrics_a_posterior,\n",
    "    'coverage_95': coverage.item(),\n",
    "}\n",
    "\n",
    "torch.save(results_to_save, 'hmc_results.pt')\n",
    "print(\"Results saved to hmc_results.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
