
run_name: "darcy_continuous"
device: "cuda"
artifact_root: "runs"
seed: 10086

problem:
  type: "darcy_continuous"
  train_data: "data/darcy_continuous/smh_train/"
  test_data: "data/darcy_continuous/smh_test_in/"

#pretrained:
#  path: "./runs/2026-01-22_16-18-13_darcy_continuous/weights/best.pt"

# Joint training configuration (encoder + decoders + NF)
training:
  epochs: 10000
  batch_size: 50
  epoch_show: 200

  # Loss weights for PDE and data terms
  # Note: NF loss doesn't need a weight (gradients don't flow to encoder/decoders)
  loss_weights:
    pde: 0.25
    data: 2.0

  # Latent standardization for NF
  # If true, latents are normalized to zero mean and unit variance before NF
  # The encoder uses Tanh output, so latents are bounded in [-1,1]^d
  # With Neural Spline Flows, standardization is typically not needed
  # IMPORTANT: Must match between training and evaluation!
  standardize_latent: false

  optimizer:
    type: Adam
    lr: 0.001
    weight_decay: 0.0

  scheduler:
    type: StepLR
    step_size: 2500
    gamma: 0.5