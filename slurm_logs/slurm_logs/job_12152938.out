==============================================
Job ID: 12152938
Job Name: darcy_continuous
Node: gpu21.daic.tudelft.net
Starting time: Mon Jan 12 19:27:50 CET 2026
==============================================
GPU Information:
Mon Jan 12 19:27:50 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A40                     On  |   00000000:81:00.0 Off |                    0 |
|  0%   32C    P8             23W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==============================================
Working directory: /home/nfs/hpage/v-igno
==============================================
Using configuration: configs/training/darcy_continuous.yaml
Additional arguments: 
==============================================
Starting training...
Command: uv run python training.py --config configs/training/darcy_continuous.yaml 
==============================================
Config: configs/training/darcy_continuous.yaml
Device: cuda, Seed: 10086
Stages: ['foundation'], Problem: darcy_continuous
Creating problem: darcy_continuous
Loading data...
  Train: a=torch.Size([1000, 841, 1]), u=torch.Size([1000, 841, 1])
  Test:  a=torch.Size([200, 841, 1]), u=torch.Size([200, 841, 1])
Setting up grids and test functions...
  int_grid: torch.Size([45, 2]), v: torch.Size([45, 1])
Building models...
  enc: 140,416 parameters
  u: 114,206 parameters
  a: 114,206 parameters
  nf: 49,920 parameters
Problem initialized.

============================================================
STAGE: FOUNDATION
============================================================
Run directory: runs/2026-01-12_19-32-43_darcy_continuous

============================================================
PHASE 1: DGNO Training
============================================================

Epoch 50: Loss=19.3785, Error=0.0549

Epoch 100: Loss=9.2083, Error=0.0293

Epoch 150: Loss=13.9431, Error=0.0280

Epoch 200: Loss=9.6216, Error=0.0338

Epoch 250: Loss=24.1750, Error=0.0489

Epoch 300: Loss=13.0706, Error=0.0322

Epoch 350: Loss=12.4601, Error=0.0312

Epoch 400: Loss=7.7683, Error=0.0279

Epoch 450: Loss=9.2972, Error=0.0205

Epoch 500: Loss=16.2252, Error=0.0361

Epoch 550: Loss=7.2487, Error=0.0308

Epoch 600: Loss=8.7941, Error=0.0240

Epoch 650: Loss=8.6046, Error=0.0298

Epoch 700: Loss=7.9949, Error=0.0314

Epoch 750: Loss=7.4319, Error=0.0199

Epoch 800: Loss=7.8262, Error=0.0256

Epoch 850: Loss=7.6856, Error=0.0223

Epoch 900: Loss=7.5500, Error=0.0274

Epoch 950: Loss=7.6976, Error=0.0183

Epoch 1000: Loss=8.4979, Error=0.0420

Epoch 1050: Loss=10.1256, Error=0.0239

Epoch 1100: Loss=7.9483, Error=0.0230

Epoch 1150: Loss=5.2554, Error=0.0174

Epoch 1200: Loss=5.9715, Error=0.0333

Epoch 1250: Loss=5.1775, Error=0.0182

Epoch 1300: Loss=5.0929, Error=0.0190

Epoch 1350: Loss=7.4139, Error=0.0208

Epoch 1400: Loss=45.9758, Error=0.0816

Epoch 1450: Loss=12.5411, Error=0.0337

Epoch 1500: Loss=10.6063, Error=0.0271

Epoch 1550: Loss=10.4765, Error=0.0306

Epoch 1600: Loss=10.9833, Error=0.0241

Epoch 1650: Loss=8.7593, Error=0.0278

Epoch 1700: Loss=10.0817, Error=0.0370

Epoch 1750: Loss=149.8065, Error=0.2238

Epoch 1800: Loss=61.1742, Error=0.0985

Epoch 1850: Loss=29.7107, Error=0.0620

Epoch 1900: Loss=21.5258, Error=0.0501

Epoch 1950: Loss=17.9166, Error=0.0475

Epoch 2000: Loss=17.0447, Error=0.0549

Epoch 2050: Loss=20.0717, Error=0.0409

Epoch 2100: Loss=17.5415, Error=0.0497

Epoch 2150: Loss=16.4213, Error=0.0652

Epoch 2200: Loss=14.1437, Error=0.0387

Epoch 2250: Loss=12.2883, Error=0.0475

Epoch 2300: Loss=13.0271, Error=0.0378

Epoch 2350: Loss=165.7638, Error=0.2473

Epoch 2400: Loss=35.1060, Error=0.0654

Epoch 2450: Loss=21.7233, Error=0.0539

Epoch 2500: Loss=17.3321, Error=0.0617

Epoch 2550: Loss=14.8850, Error=0.0445

Epoch 2600: Loss=11.7714, Error=0.0351

Epoch 2650: Loss=13.0050, Error=0.0365

Epoch 2700: Loss=9.4288, Error=0.0298

Epoch 2750: Loss=9.7545, Error=0.0352

Epoch 2800: Loss=9.3100, Error=0.0258

Epoch 2850: Loss=12.0636, Error=0.0329

Epoch 2900: Loss=9.9565, Error=0.0414

Epoch 2950: Loss=11.5317, Error=0.0305

Epoch 3000: Loss=768.7798, Error=0.2590

Epoch 3050: Loss=20.4288, Error=0.0477

Epoch 3100: Loss=14.7281, Error=0.0408

Epoch 3150: Loss=11.1335, Error=0.0394

Epoch 3200: Loss=15.9375, Error=0.0523

Epoch 3250: Loss=16.4484, Error=0.0671

Epoch 3300: Loss=10.0830, Error=0.0264

Epoch 3350: Loss=8.6465, Error=0.0487

Epoch 3400: Loss=10.3615, Error=0.0425

Epoch 3450: Loss=8.7075, Error=0.0331

Epoch 3500: Loss=25.2469, Error=0.0707

Epoch 3550: Loss=13.6106, Error=0.0413

Epoch 3600: Loss=12.7183, Error=0.0448

Epoch 3650: Loss=8.9423, Error=0.0354

Epoch 3700: Loss=9.9282, Error=0.0374

Epoch 3750: Loss=7.3495, Error=0.0285

Epoch 3800: Loss=9.6319, Error=0.0309

Epoch 3850: Loss=8.5572, Error=0.0354

Epoch 3900: Loss=5.8449, Error=0.0258

Epoch 3950: Loss=5.5994, Error=0.0182

Epoch 4000: Loss=6.6527, Error=0.0206

Epoch 4050: Loss=5.6756, Error=0.0202

Epoch 4100: Loss=5.3076, Error=0.0174

Epoch 4150: Loss=5.6185, Error=0.0439

Epoch 4200: Loss=5.3860, Error=0.0243

Epoch 4250: Loss=5.8964, Error=0.0281

Epoch 4300: Loss=5.3878, Error=0.0199

Epoch 4350: Loss=3.7023, Error=0.0156

Epoch 4400: Loss=4.4878, Error=0.0186

Epoch 4450: Loss=4.0649, Error=0.0158

Epoch 4500: Loss=5.8533, Error=0.0349

Epoch 4550: Loss=5.1233, Error=0.0215

Epoch 4600: Loss=5.7211, Error=0.0209

Epoch 4650: Loss=3.1508, Error=0.0135

Epoch 4700: Loss=4.8859, Error=0.0189

Epoch 4750: Loss=4.3959, Error=0.0196

Epoch 4800: Loss=3.6419, Error=0.0133

Epoch 4850: Loss=3.2782, Error=0.0157

Epoch 4900: Loss=3.3151, Error=0.0154

Epoch 4950: Loss=3.7604, Error=0.0247

Epoch 5000: Loss=2.7945, Error=0.0112

Epoch 5050: Loss=3.4977, Error=0.0159

Epoch 5100: Loss=1.5583, Error=0.0100

Epoch 5150: Loss=2.5515, Error=0.0162

Epoch 5200: Loss=1.8936, Error=0.0113

Epoch 5250: Loss=3.8917, Error=0.0146

Epoch 5300: Loss=1.7847, Error=0.0075

Epoch 5350: Loss=1.4977, Error=0.0071

Epoch 5400: Loss=2.7138, Error=0.0157

Epoch 5450: Loss=15.2818, Error=0.0739

Epoch 5500: Loss=1.3625, Error=0.0122

Epoch 5550: Loss=2.8463, Error=0.0102

Epoch 5600: Loss=2.7397, Error=0.0176

Epoch 5650: Loss=3.7060, Error=0.0173

Epoch 5700: Loss=1.6382, Error=0.0087

Epoch 5750: Loss=1.2627, Error=0.0061

Epoch 5800: Loss=2.7678, Error=0.0101

Epoch 5850: Loss=1.6148, Error=0.0181

Epoch 5900: Loss=4.9432, Error=0.0183

Epoch 5950: Loss=1.6149, Error=0.0082

Epoch 6000: Loss=2.0000, Error=0.0109

Epoch 6050: Loss=2.1436, Error=0.0093

Epoch 6100: Loss=1.6298, Error=0.0094

Epoch 6150: Loss=2.1352, Error=0.0088

Epoch 6200: Loss=1.3950, Error=0.0079

Epoch 6250: Loss=1.2811, Error=0.0075

Epoch 6300: Loss=1.8281, Error=0.0093

Epoch 6350: Loss=2.5514, Error=0.0089

Epoch 6400: Loss=2.1122, Error=0.0097

Epoch 6450: Loss=1.1717, Error=0.0078

Epoch 6500: Loss=1.0320, Error=0.0055

Epoch 6550: Loss=1.6391, Error=0.0140

Epoch 6600: Loss=2.7701, Error=0.0105

Epoch 6650: Loss=2.2770, Error=0.0079

Epoch 6700: Loss=1.3367, Error=0.0082

Epoch 6750: Loss=1.1879, Error=0.0091

Epoch 6800: Loss=1.8803, Error=0.0261

Epoch 6850: Loss=1.5891, Error=0.0124

Epoch 6900: Loss=3.5427, Error=0.0218

Epoch 6950: Loss=1.7822, Error=0.0101

Epoch 7000: Loss=1.3658, Error=0.0099

Epoch 7050: Loss=1.9396, Error=0.0123

Epoch 7100: Loss=1.4162, Error=0.0068

Epoch 7150: Loss=1.7456, Error=0.0080

Epoch 7200: Loss=1.9196, Error=0.0142

Epoch 7250: Loss=7.4479, Error=0.0391

Epoch 7300: Loss=1.3059, Error=0.0095

Epoch 7350: Loss=1.8647, Error=0.0098

Epoch 7400: Loss=2.2925, Error=0.0107

Epoch 7450: Loss=1.3837, Error=0.0080

Epoch 7500: Loss=1.7826, Error=0.0070

Epoch 7550: Loss=0.9047, Error=0.0055

Epoch 7600: Loss=1.4954, Error=0.0071

Epoch 7650: Loss=0.9551, Error=0.0073

Epoch 7700: Loss=0.9640, Error=0.0053

Epoch 7750: Loss=2.4378, Error=0.0098

Epoch 7800: Loss=0.7717, Error=0.0073

Epoch 7850: Loss=0.6915, Error=0.0068

Epoch 7900: Loss=0.6863, Error=0.0061

Epoch 7950: Loss=1.1144, Error=0.0071

Epoch 8000: Loss=0.6732, Error=0.0048

Epoch 8050: Loss=1.1698, Error=0.0196

Epoch 8100: Loss=0.9524, Error=0.0101

Epoch 8150: Loss=1.1539, Error=0.0060

Epoch 8200: Loss=1.1895, Error=0.0056

Epoch 8250: Loss=2.7284, Error=0.0172

Epoch 8300: Loss=1.5374, Error=0.0091

Epoch 8350: Loss=0.6800, Error=0.0057

Epoch 8400: Loss=0.7382, Error=0.0049

Epoch 8450: Loss=0.6974, Error=0.0054

Epoch 8500: Loss=1.7970, Error=0.0092

Epoch 8550: Loss=0.7915, Error=0.0056

Epoch 8600: Loss=1.0904, Error=0.0086

Epoch 8650: Loss=0.6290, Error=0.0066

Epoch 8700: Loss=1.2062, Error=0.0071

Epoch 8750: Loss=0.8323, Error=0.0090

Epoch 8800: Loss=0.6565, Error=0.0048

Epoch 8850: Loss=0.9564, Error=0.0054

Epoch 8900: Loss=1.8289, Error=0.0066

Epoch 8950: Loss=0.5611, Error=0.0052

Epoch 9000: Loss=0.9185, Error=0.0080

Epoch 9050: Loss=0.7035, Error=0.0073

Epoch 9100: Loss=1.1510, Error=0.0077

Epoch 9150: Loss=0.5719, Error=0.0055

Epoch 9200: Loss=1.0692, Error=0.0094

Epoch 9250: Loss=0.7238, Error=0.0092

Epoch 9300: Loss=0.5020, Error=0.0046

Epoch 9350: Loss=0.6109, Error=0.0088

Epoch 9400: Loss=0.5558, Error=0.0047

Epoch 9450: Loss=0.6737, Error=0.0052

Epoch 9500: Loss=0.7716, Error=0.0075

Epoch 9550: Loss=0.7926, Error=0.0064

Epoch 9600: Loss=0.7044, Error=0.0097

Epoch 9650: Loss=1.5741, Error=0.0123

Epoch 9700: Loss=2.4006, Error=0.0179

Epoch 9750: Loss=1.1815, Error=0.0085

Epoch 9800: Loss=0.8535, Error=0.0050

Epoch 9850: Loss=0.4841, Error=0.0068

Epoch 9900: Loss=0.7612, Error=0.0070

Epoch 9950: Loss=0.7607, Error=0.0055

Epoch 10000: Loss=0.6667, Error=0.0064

Reloading best DGNO from: runs/2026-01-12_19-32-43_darcy_continuous/foundation/weights/best_dgno.pt
Loading checkpoint: runs/2026-01-12_19-32-43_darcy_continuous/foundation/weights/best_dgno.pt
  Loaded: enc
  Loaded: u
  Loaded: a

============================================================
PHASE 2: NF Training
============================================================
Extracting latents...

Epoch 100: Train NLL=-265.6192, Test NLL=-265.7076

Epoch 200: Train NLL=-266.0255, Test NLL=-265.6310

Epoch 300: Train NLL=-266.2126, Test NLL=-266.2214

Epoch 400: Train NLL=-266.2619, Test NLL=-266.2774

Epoch 500: Train NLL=-266.2613, Test NLL=-266.2976

Epoch 600: Train NLL=-266.3238, Test NLL=-266.3267

Epoch 700: Train NLL=-266.3384, Test NLL=-266.3356

Epoch 800: Train NLL=-266.3455, Test NLL=-266.3463

Epoch 900: Train NLL=-266.3463, Test NLL=-266.3461

Epoch 1000: Train NLL=-266.3485, Test NLL=-266.3491

Epoch 1100: Train NLL=-266.2180, Test NLL=-266.2720

Epoch 1200: Train NLL=-266.3396, Test NLL=-266.3457

Epoch 1300: Train NLL=-266.3469, Test NLL=-266.3266

Epoch 1400: Train NLL=-266.3495, Test NLL=-266.3496

Epoch 1500: Train NLL=-266.3542, Test NLL=-266.3500

Epoch 1600: Train NLL=-266.3434, Test NLL=-266.3420

Epoch 1700: Train NLL=-266.3475, Test NLL=-266.3483

Epoch 1800: Train NLL=-266.3510, Test NLL=-266.3543

Epoch 1900: Train NLL=-266.3545, Test NLL=-266.3587

Epoch 2000: Train NLL=-266.3586, Test NLL=-266.3565

Epoch 2100: Train NLL=-266.3508, Test NLL=-266.3536

Epoch 2200: Train NLL=-266.3516, Test NLL=-266.3542

Epoch 2300: Train NLL=-266.3613, Test NLL=-266.3618

Epoch 2400: Train NLL=-266.3462, Test NLL=-266.3512

Epoch 2500: Train NLL=-266.3387, Test NLL=-266.3466

Epoch 2600: Train NLL=-266.3637, Test NLL=-266.3640

Epoch 2700: Train NLL=-266.3639, Test NLL=-266.3638

Epoch 2800: Train NLL=-266.3620, Test NLL=-266.3610

Epoch 2900: Train NLL=-266.3645, Test NLL=-266.3654

Epoch 3000: Train NLL=-266.3649, Test NLL=-266.3644

Epoch 3100: Train NLL=-266.3657, Test NLL=-266.3657

Epoch 3200: Train NLL=-266.3651, Test NLL=-266.3651

Epoch 3300: Train NLL=-266.3655, Test NLL=-266.3643

Epoch 3400: Train NLL=-266.3665, Test NLL=-266.3665

Epoch 3500: Train NLL=-266.3665, Test NLL=-266.3673

Epoch 3600: Train NLL=-266.3658, Test NLL=-266.3652

Epoch 3700: Train NLL=-266.3674, Test NLL=-266.3665

Epoch 3800: Train NLL=-266.3662, Test NLL=-266.3666

Epoch 3900: Train NLL=-266.3602, Test NLL=-266.3550

Epoch 4000: Train NLL=-266.3650, Test NLL=-266.3642

Epoch 4100: Train NLL=-266.3665, Test NLL=-266.3634

Epoch 4200: Train NLL=-266.3669, Test NLL=-266.3672

Epoch 4300: Train NLL=-266.3676, Test NLL=-266.3683

Epoch 4400: Train NLL=-266.3669, Test NLL=-266.3671

Epoch 4500: Train NLL=-266.3661, Test NLL=-266.3663

Epoch 4600: Train NLL=-266.3663, Test NLL=-266.3648

Epoch 4700: Train NLL=-266.3680, Test NLL=-266.3680

Epoch 4800: Train NLL=-266.3682, Test NLL=-266.3685

Epoch 4900: Train NLL=-266.3468, Test NLL=-266.3591

Epoch 5000: Train NLL=-266.3670, Test NLL=-266.3668

Epoch 5100: Train NLL=-266.3690, Test NLL=-266.3690

Epoch 5200: Train NLL=-266.3695, Test NLL=-266.3684

Epoch 5300: Train NLL=-266.3697, Test NLL=-266.3698

Epoch 5400: Train NLL=-266.3694, Test NLL=-266.3692

Epoch 5500: Train NLL=-266.3689, Test NLL=-266.3689

Epoch 5600: Train NLL=-266.3699, Test NLL=-266.3695

Epoch 5700: Train NLL=-266.3680, Test NLL=-266.3694

Epoch 5800: Train NLL=-266.3704, Test NLL=-266.3703

Epoch 5900: Train NLL=-266.3699, Test NLL=-266.3698

Epoch 6000: Train NLL=-266.3705, Test NLL=-266.3705

Epoch 6100: Train NLL=-266.3694, Test NLL=-266.3688

Epoch 6200: Train NLL=-266.3694, Test NLL=-266.3685

Epoch 6300: Train NLL=-266.3699, Test NLL=-266.3700

Epoch 6400: Train NLL=-266.3703, Test NLL=-266.3703

Epoch 6500: Train NLL=-266.3702, Test NLL=-266.3705

Epoch 6600: Train NLL=-266.3705, Test NLL=-266.3705

Epoch 6700: Train NLL=-266.3692, Test NLL=-266.3681

Epoch 6800: Train NLL=-266.3702, Test NLL=-266.3701

Epoch 6900: Train NLL=-266.3673, Test NLL=-266.3689

Epoch 7000: Train NLL=-266.3704, Test NLL=-266.3693

Epoch 7100: Train NLL=-266.3702, Test NLL=-266.3704

Epoch 7200: Train NLL=-266.3702, Test NLL=-266.3703

Epoch 7300: Train NLL=-266.3698, Test NLL=-266.3703

Epoch 7400: Train NLL=-266.3704, Test NLL=-266.3704

Epoch 7500: Train NLL=-266.3705, Test NLL=-266.3705

Epoch 7600: Train NLL=-266.3708, Test NLL=-266.3705

Epoch 7700: Train NLL=-266.3710, Test NLL=-266.3709

Epoch 7800: Train NLL=-266.3707, Test NLL=-266.3705

Epoch 7900: Train NLL=-266.3709, Test NLL=-266.3706

Epoch 8000: Train NLL=-266.3711, Test NLL=-266.3707

Epoch 8100: Train NLL=-266.3713, Test NLL=-266.3713

Epoch 8200: Train NLL=-266.3705, Test NLL=-266.3706

Epoch 8300: Train NLL=-266.3713, Test NLL=-266.3713

Epoch 8400: Train NLL=-266.3715, Test NLL=-266.3714

Epoch 8500: Train NLL=-266.3710, Test NLL=-266.3712

Epoch 8600: Train NLL=-266.3715, Test NLL=-266.3713

Epoch 8700: Train NLL=-266.3714, Test NLL=-266.3710

Epoch 8800: Train NLL=-266.3713, Test NLL=-266.3710

Epoch 8900: Train NLL=-266.3716, Test NLL=-266.3714

Epoch 9000: Train NLL=-266.3714, Test NLL=-266.3712

Epoch 9100: Train NLL=-266.3715, Test NLL=-266.3714

Epoch 9200: Train NLL=-266.3713, Test NLL=-266.3712

Epoch 9300: Train NLL=-266.3716, Test NLL=-266.3714

Epoch 9400: Train NLL=-266.3712, Test NLL=-266.3706

Epoch 9500: Train NLL=-266.3716, Test NLL=-266.3714

Epoch 9600: Train NLL=-266.3710, Test NLL=-266.3710

Epoch 9700: Train NLL=-266.3713, Test NLL=-266.3712

Epoch 9800: Train NLL=-266.3715, Test NLL=-266.3714

Epoch 9900: Train NLL=-266.3712, Test NLL=-266.3710

Epoch 10000: Train NLL=-266.3717, Test NLL=-266.3716

Checkpoints saved to: runs/2026-01-12_19-32-43_darcy_continuous/foundation/weights

Done. Results: runs/2026-01-12_19-32-43_darcy_continuous
==============================================
Training completed with exit status: 0
End time: Tue Jan 13 04:46:16 CET 2026
==============================================
==============================================
Job 12152938 finished
==============================================
